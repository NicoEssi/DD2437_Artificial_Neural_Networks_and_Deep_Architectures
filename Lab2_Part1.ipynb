{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Radial_Basis_Function-Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/GZcuWuHpk+ZQP/bgVBCo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoEssi/DD2437_Artificial_Neural_Networks_and_Deep_Architectures/blob/master/Lab2_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IniL8TUkFvmn",
        "colab_type": "text"
      },
      "source": [
        "# DD2437 - Lab 2, Part 1\n",
        "### Authored by Nicolas Essipova\n",
        "\n",
        "## Scope\n",
        "\n",
        "In this lab, I will build and study a radial basis function neural network (also known as RBF network) in application to one- and two-dimensional function approximation problems with Gaussian noise. To automate the process of RBF unit initialisation, I will also develop a simple competitive learning algorithm.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "* know how to build the structure and perform training of an RBF network\n",
        "for either classification or regression purposes.\n",
        "\n",
        "* be able to comparatively analyse different methods for initialising the\n",
        "structure and learning the weights in an RBF network.\n",
        "\n",
        "* know the concept of vector quantisation and learn how to use it in NN\n",
        "context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh-Wfy4zD0Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies\n",
        "import math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.cluster import KMeans\n",
        "from numpy.linalg import pinv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A30LfiCBFwML",
        "colab_type": "text"
      },
      "source": [
        "# The Radial Basis Function Neural Network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn97yZI5FwmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class node:\n",
        "\n",
        "  def __init__(self, sigma, dimensions):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "    sigma - integer: variance of our node\n",
        "    dimensions - integer: dimensions of our node\n",
        "\n",
        "    OUTPUT:\n",
        "    params - array: centroid in n-dimensions, along with variance\n",
        "    \"\"\"\n",
        "    self.sigma = sigma\n",
        "    self.dimensions = dimensions\n",
        "    self.params = [[random.uniform(0, 10) for i in range(dimensions)], sigma]\n",
        "\n",
        "\n",
        "  def distance(self, x):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "    x - numpy array: input features\n",
        "\n",
        "    OUTPUT:\n",
        "    c - summed distances between features and node squared (*)\n",
        "\n",
        "    * : squared relevant for our transfer function\n",
        "    \"\"\"\n",
        "    c = 0\n",
        "    for i in range(len(x)):\n",
        "      c += (x[i] - self.params[0][i]) ** 2\n",
        "    return c\n",
        "\n",
        "\n",
        "  def phi(self, x):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "    x - numpy array: input features\n",
        "\n",
        "    OUTPUT:\n",
        "    phix - activation output\n",
        "    \"\"\"\n",
        "    phix = math.e ** (-(self.distance(x)) / \\\n",
        "                   (2 * (self.param[i] ** 2)))\n",
        "    return phix\n",
        "\n",
        "\n",
        "  def update(self, x, eta):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    for i in range(self.dimensions):\n",
        "      self.param[0][i] += eta * (x[i] - self.param[0][i])\n",
        "\n",
        "  def dim_match(x):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "    x - numpy array: input features\n",
        "\n",
        "    OUTPUT:\n",
        "    check - boolean: True if dimensions match, otherwise False.\n",
        "    \"\"\"\n",
        "    check = len(x) == node.dimensions\n",
        "    return check"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkrpiggrYzUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class network:\n",
        "  \n",
        "  # Initialize the radial basis function neural network\n",
        "  def __init__(self, nodes):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    self.nodes = [node() for _ in range(nodes)]\n",
        "    self.weights = np.array([[random.uniform(0,1) for _ in range(nodes)]]).T\n",
        "\n",
        "\n",
        "  # Forward propagation through radial basis network\n",
        "  def forwardr(self, X):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    phi = np.zeros((len(X), len(self.nodes)))\n",
        "    \n",
        "    for i in range(len(X)):\n",
        "\n",
        "      for j in range(len(self.nodes)):\n",
        "        \n",
        "        phi[i][j] = self.nodes[j].distance(X[i])\n",
        "    \n",
        "    predictions = self.forwardn(phi)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "  # Forward propagation through neural network\n",
        "  def forwardn(self, rbf_out):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    nn_out = np.dot(rbf_out, self.weights).T\n",
        "    return nn_out.tolist()[0]\n",
        "\n",
        "\n",
        "  # Forward propagation for least squared\n",
        "  def forwards(self, X):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    predictions = self.forwardr(X)\n",
        "\n",
        "    for i in range(len(X)):\n",
        "      if predictions[i] > 0:\n",
        "        predictions[i] = 1\n",
        "      else:\n",
        "        predictions[i] = -1\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "\n",
        "  # Calculate the error between labels and targets\n",
        "  def error(self, predictions, targets):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    sum_error = 0\n",
        "    for i in range(len(predictions)):\n",
        "      sum_error += abs(predictions[i] - targets[i])\n",
        "    avg_error = c/len(predictions)\n",
        "    return avg_error\n",
        "\n",
        "  # ------------------------------------------------------------------ #\n",
        "  # ---------------------- Training algorithms ----------------------- #\n",
        "  # ------------------------------------------------------------------ #\n",
        "\n",
        "  # Training with Least Squares\n",
        "  def train_ls(self, X, Y):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    Y = np.array([Y])\n",
        "    phi = np.zeros((len(X), len(self.nodes)))\n",
        "\n",
        "    for i in range(len(X)):\n",
        "      for j in range(len(self.nodes)):\n",
        "        phi[i][j] = self.nodes[j].distance(X[i])\n",
        "\n",
        "    self.weights = np.dot(np.dot(np.dot(pinv(phi), pinv(phi.T)), phi.T), Y.T)\n",
        "  \n",
        "\n",
        "  # Training with Delta Rule\n",
        "  def train_dr(self, X, Y, lr, epochs, batch):\n",
        "    \"\"\"\n",
        "    INPUTS:\n",
        "    X - \n",
        "    Y - \n",
        "    lr - \n",
        "    epochs - \n",
        "    batch -\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Iterating over each epoch\n",
        "    for epoch in range(epochs):\n",
        "      samples = random.sample(range(len(X)), batch)\n",
        "      delta = np.seros((len(self.nodes), 1))\n",
        "\n",
        "      # For each row of data in the batch\n",
        "      for data in range(batch):\n",
        "        index = samples[data]\n",
        "        distances = np.zeros((len(self.nodes), 1))\n",
        "\n",
        "        # Calculate the distances to each node\n",
        "        for i in range(len(self.nodes)):\n",
        "          distances[i][0] = self.nodes[i].distance(X[index])\n",
        "        \n",
        "        # Compute the predicted target\n",
        "        pred_Y = np.dot(distances.T, self.weights)\n",
        "\n",
        "        # Compute the error between prediction and target \n",
        "        e = Y[index] - pred_Y\n",
        "\n",
        "        # Delta rule\n",
        "        delta += lr * e * distances\n",
        "\n",
        "      # Adjust our weights with delta rule\n",
        "      self.weights += delta / batch\n",
        "\n",
        "    \n",
        "  # Training with Competitive Learning\n",
        "  def train_cl(self, X, Y, lr, epochs, deadNodes = True, coWinners = 5):\n",
        "\n",
        "    if deadNodes == False:\n",
        "      for epoch in range(epochs):\n",
        "        dists, index = self._compute_cl(X, deadNodes)\n",
        "        self.nodes[np.argmin(dists)].update(X[index], lr)\n",
        "\n",
        "    else:\n",
        "      for epoch in range(epochs):\n",
        "        dists, index = self._compute_cl(X, deadNodes)\n",
        "        self.nodes[dists[0]].update(X[index], lr)\n",
        "\n",
        "        for i in range(1, 1 + coWinners):\n",
        "          self.nodes[dists[i]].update(X[index], lr / 5)\n",
        "\n",
        "\n",
        "  def _compute_cl(self, X, deadNodes):\n",
        "    index = random.randint(0, len(X) - 1)\n",
        "    dists = np.array([self.nodes[i].dist(X[index]) for i in range(len(self.nodes))])\n",
        "    if deadNodes:\n",
        "      dists = np.argsort(dists)\n",
        "    return dists, index\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MRf8C0JBni2",
        "colab_type": "text"
      },
      "source": [
        "# Data Generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsUq5flaFlbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(datatype, step=0.1, start=0, noise=0):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    if datatype = \"sin\":\n",
        "        while start < math.pi * 2:\n",
        "            x.append([start])\n",
        "            y.append(mfsin(start, noise))\n",
        "            start = start + step\n",
        "\n",
        "    if datatype = \"square\":\n",
        "        while start < math.pi * 2:\n",
        "            x.append([start])\n",
        "            if mfsin(start, noise) > 0:\n",
        "                y.append(1)\n",
        "            else:\n",
        "                y.append(-1)\n",
        "            start = start + step\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def fsin(x, noise):\n",
        "  y = math.sin(2 * x) + random.normalvariate(0, noise)\n",
        "  return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAKePYZIIazz",
        "colab_type": "text"
      },
      "source": [
        "# Batch mode training using least squares - supervised learning of network weights (needs editing)\n",
        "\n",
        "In this simple assignment, I focus on the supervised learning of weights of the RBF network built to address a simple regression problem. I've already implemented batch learning from scratch without using any dedicated neural network toolboxes. The two function to approximare are sin(2x) and square(2x) (square is a rectangular curve serving as a \"box\" envelope for the sine wave, i.e. it is 1 for\n",
        "arguments where the sin>=0 and -1 otherwise) are also defined. Note that the input space is R, so each pattern x1, x2, . . . , xN in (6) is in fact a scalar. They create a column vector containing the points (patterns) where I want to evaluate my function. Let's limit the regression to the interval [0, 2Ï€]. Sample this interval starting from 0 with the step size of 0.1 and calculate the values of the two functions at the these points to obtain the corresponding training sets. The testing sets could be generated analogously with sampling starting from 0.05 and the same step size. For a varying number of RBF nodes, please place them by hand in the input space according to your judgement, and set the same variance to each node. Next, apply your batch learning algorihtm on your training set to adjust the output weights and test accordingly on the hold-out set. For both functions (studied indpenedently), please consider and discuss the following issues (which involve running the suggested experiments):\n",
        "\n",
        "* Try to vary the number of units to obtain the absolute residual error\n",
        "below 0.1, 0.01 and 0.001 in the residual value (absolute residual error is\n",
        "understood as the average absolute di\u001berence between the network outputs\n",
        "and the desirable target values). Please discuss the results, how many units\n",
        "are needed for the aforementioned error thresholds?\n",
        "\n",
        "* How can you simply transform the output of your RBF network to reduce the residual error to 0 for the square(2x) problem? Still, how many\n",
        "units do you need? In what type of applications could this transform be\n",
        "particularly useful?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YPzJgE-IdUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUmx7ACSJ4C_",
        "colab_type": "text"
      },
      "source": [
        "# Regression with noise (needs editing)\n",
        "\n",
        "Please add zero-mean Gaussian noise with the variance of 0.1 to both training\n",
        "and testing datasets for the same functions as in section 3.1. You are also\n",
        "requested to implement an algorithm for on-line learning with delta rule and\n",
        "compare it with batch learning developed in section 3.1 (given that the data\n",
        "points are randomly shuffled in each epoch). As before, please place the RBF\n",
        "units by hand wherever you consider suitable and \u001cx the same width for them.\n",
        "Vary the number of RBF nodes within reasonable limits and try \u001cve di\u001berent\n",
        "widths of the RBF kernel to investigate the e\u001bect of these two parameters on\n",
        "the regression performance in noisy conditions. Please consider the following\n",
        "points and make the requsted analyses:\n",
        "\n",
        "* Compare the e\u001bect of the number of RBF units and their width for the\n",
        "two learning approaches. Which error estimate should you choose as the\n",
        "criterion for these comparative analyses?\n",
        "\n",
        "* What can you say about the rate of convergence and its dependence on\n",
        "the learning rate, eta, for the two learning schemes?\n",
        "\n",
        "* What are the main e\u001bets of changing the width of RBFs?\n",
        "\n",
        "* How important is the positioning of the RBF nodes in the input space?\n",
        "What strategy did you choose? Is it better than random positioning of the\n",
        "RBF nodes? Please support your conclusions with quantitative evidence\n",
        "(e.g., error comparison).\n",
        "\n",
        "* Also, for the same network models estimate their test performance on the\n",
        "original clean data used in section 3.1 (a corresponding test subset but\n",
        "without noise) and compare your findings.\n",
        "\n",
        "* Please compare your optimal RBF network trained in batch mode with\n",
        "a single-hidden-layer perceptron trained with backprop (also in batch\n",
        "mode), which you implemented in the \u001crst lab assignment. Please use\n",
        "the same number of hidden units as in the RBF network. The comparison\n",
        "should be made for both functions: sin(2x) and square(2x), only for the\n",
        "noisy case. Please remember that generalisation performance and training\n",
        "time are of greatest interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgZUPOEcKLu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cthyUOzKNIR",
        "colab_type": "text"
      },
      "source": [
        "# Competitive learning (CL) to initialise RBF units\n",
        "\n",
        "Now we will take a look at the problem of placing the RBFs in input space. We\n",
        "will use a version of CL for Vector Quantization. A simple algorithm we use\n",
        "here can only adjust the positions of the RBF units without adjusting the width\n",
        "of the units. Therefore you will have to make these adjustment yourselves based\n",
        "on the distribution of data around the cluster centers found with this simple CL\n",
        "algorithm. At each iteration of CL a training vector is randomly selected from\n",
        "the data. The closest RBF unit (usually called the winning unit) is computed,\n",
        "and this unit is updated, in such a way that it gets closer to the training vector. The other units may or may not (depending on the version of CL used) be\n",
        "moved towards it too, depending on distance. This way the units will tend\n",
        "to aggregate in the clusters in the data space. Please, couple the CL-based\n",
        "approach to RBF network initilisation with the aforementioned delta learning\n",
        "for the output weights.\n",
        "\n",
        "* Compare the CL-based approach with your earlier RBF network where\n",
        "you manually positioned RBF nodes in the input space. Make this comparison for both noise-free and noisy approximation of sin(2x) and use\n",
        "the number of units corresponding to the best performing architectures\n",
        "found in sections 3.1 and 3.2, respectively. Pay attention to convergence,\n",
        "generalisation performance and the resulting position of nodes.\n",
        "\n",
        "* Introduce a strategy to avoid dead units, e.g. by having more than a single\n",
        "winner. Choose an example to demonstrate this e\u001bect in comparison with\n",
        "the vanilla version of our simple CL algorithm.\n",
        "\n",
        "* Con\u001cgure an RBF network with the use of CL for positioning the RBF\n",
        "units to approximate a two-dimensional function, i.e. from R\n",
        "2\n",
        "to R\n",
        "2\n",
        ". As\n",
        "training examples please use noisy data from ballistical experiments where\n",
        "inputs are pairs: <angle, velocity> and the outputs are pairs: <distance, height>. There are two datasets available: ballist for training\n",
        "and balltest for testing. First thing to do is to load the data and then\n",
        "train the RBF network to \u001cnd a mapping between the input and output values. Please be careful with the selection of a suitable number of\n",
        "nodes and their initialisation to avoid dead-unit and over\u001ctting problems.\n",
        "Report your results and observations, ideally with the support of illustrations, and document your analyses (e.g., inspect the position of units in\n",
        "the input space).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkYUBL_AK2s6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}